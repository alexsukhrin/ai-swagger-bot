"""
RAG –¥–≤–∏–≥—É–Ω –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ API endpoints.
"""

import logging
import os
from typing import Any, Dict, List, Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings

from src.enhanced_swagger_parser import EnhancedSwaggerParser
from src.postgres_vector_manager import PostgresVectorManager

logger = logging.getLogger(__name__)


class PostgresRAGEngine:
    """RAG –¥–≤–∏–≥—É–Ω –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º PostgreSQL —Ç–∞ pgvector."""

    def __init__(self, user_id: str, swagger_spec_id: str, config: Dict[str, Any] = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è PostgreSQL RAG –¥–≤–∏–≥—É–Ω–∞.

        Args:
            user_id: ID –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            swagger_spec_id: ID Swagger —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
            config: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è RAG
        """
        from src.config import Config

        self.user_id = user_id
        self.swagger_spec_id = swagger_spec_id
        self.vector_manager = PostgresVectorManager()

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∞–±–æ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        if config:
            chunk_size = config.get("chunk_size", 1000)
            chunk_overlap = config.get("chunk_overlap", 200)
        else:
            chunk_size = 1000
            chunk_overlap = 200

        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap, separators=["\n\n", "\n", " ", ""]
        )

        logger.info(f"–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è PostgreSQL RAG Engine –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {user_id}")

    def create_vectorstore_from_swagger(
        self, swagger_spec_path: str, enable_gpt_enhancement: bool = True
    ) -> bool:
        """
        –°—Ç–≤–æ—Ä—é—î –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –∑ Swagger —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.
        –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î GPT –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ embeddings.

        Args:
            swagger_spec_path: –®–ª—è—Ö –¥–æ Swagger —Ñ–∞–π–ª—É
            enable_gpt_enhancement: –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ GPT –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è

        Returns:
            True —è–∫—â–æ —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ
        """
        try:
            logger.info("–ü–∞—Ä—Å–∏–Ω–≥ Swagger —Å–ø–µ—Ü–∏—Ñ—ñ–∫–∞—Ü—ñ—ó...")
            # –ü–∞—Ä—Å–∏–º–æ Swagger —Ñ–∞–π–ª
            parser = EnhancedSwaggerParser(swagger_spec_path)

            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è chunks
            chunks = parser.create_enhanced_endpoint_chunks()
            logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–æ {len(chunks)} –±–∞–∑–æ–≤–∏—Ö chunks")

            # GPT –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
            if enable_gpt_enhancement:
                try:
                    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î swagger_data —É parser
                    if hasattr(parser, "swagger_data") and parser.swagger_data:
                        enhanced_chunks, gpt_prompts = self._enhance_chunks_with_gpt(
                            chunks, parser.swagger_data
                        )
                        if enhanced_chunks and gpt_prompts:
                            chunks = enhanced_chunks
                            logger.info(
                                f"‚ú® –ü–æ–∫—Ä–∞—â–µ–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é GPT: {len(chunks)} chunks, {len(gpt_prompts)} –ø—Ä–æ–º–ø—Ç—ñ–≤"
                            )

                            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ GPT-–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏
                            self._save_gpt_prompts(gpt_prompts)
                        else:
                            logger.warning(
                                "‚ö†Ô∏è GPT enhancement –Ω–µ –¥–∞–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤—ñ chunks"
                            )
                    else:
                        logger.warning("‚ö†Ô∏è Swagger data –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è GPT –∞–Ω–∞–ª—ñ–∑—É")
                except Exception as gpt_error:
                    logger.warning(
                        f"‚ö†Ô∏è GPT enhancement –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: {gpt_error}. –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –±–∞–∑–æ–≤–∏–º–∏ chunks"
                    )
                    # –ü—Ä–æ–¥–æ–≤–∂—É—î–º–æ –∑ –±–∞–∑–æ–≤–∏–º–∏ chunks

            # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É
            self.create_vectorstore(chunks)
            logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")
            return True

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏: {e}")
            return False

    def create_vectorstore(self, chunks: List[Dict[str, Any]]) -> None:
        """
        –°—Ç–≤–æ—Ä—é—î –≤–µ–∫—Ç–æ—Ä–Ω—É –±–∞–∑—É –¥–∞–Ω–∏—Ö –∑ chunks –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

        Args:
            chunks: –°–ø–∏—Å–æ–∫ chunks –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        """
        for chunk in chunks:
            try:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç—É
                embedding = self.embeddings.embed_query(chunk["text"])

                # –î–æ–¥–∞—î–º–æ –≤ PostgreSQL –∑ –ø—Ä–∏–≤'—è–∑–∫–æ—é –¥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ full_url (base URL + path) –∑–∞–º—ñ—Å—Ç—å —Ç—ñ–ª—å–∫–∏ path
                endpoint_path = chunk["metadata"].get("full_url", chunk["metadata"].get("path", ""))
                self.vector_manager.add_embedding(
                    user_id=self.user_id,
                    swagger_spec_id=self.swagger_spec_id,
                    endpoint_path=endpoint_path,
                    method=chunk["metadata"].get("method", "GET"),
                    description=chunk["text"],
                    embedding=embedding,
                    metadata=chunk["metadata"],
                )
            except Exception as e:
                logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä–∞: {e}")
                continue

    def search_similar_endpoints(self, query: str, limit: int = 3) -> List[Dict[str, Any]]:
        """
        –®—É–∫–∞—î –ø–æ–¥—ñ–±–Ω—ñ endpoints –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

        Args:
            query: –ü–æ—à—É–∫–æ–≤–∏–π –∑–∞–ø–∏—Ç
            limit: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö endpoints
        """
        try:
            # –°—Ç–≤–æ—Ä—é—î–º–æ –µ–º–±–µ–¥—ñ–Ω–≥ –¥–ª—è –∑–∞–ø–∏—Ç—É
            query_embedding = self.embeddings.embed_query(query)

            # –®—É–∫–∞—î–º–æ –ø–æ–¥—ñ–±–Ω—ñ –≤–µ–∫—Ç–æ—Ä–∏
            results = self.vector_manager.search_similar(
                query_embedding=query_embedding,
                user_id=self.user_id,
                swagger_spec_id=self.swagger_spec_id,
                limit=limit,
            )

            logger.info(
                f"üîç –ó–Ω–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ–¥—ñ–±–Ω–∏—Ö endpoints –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}"
            )
            return results

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É endpoints: {e}")
            return []

    def get_all_endpoints(self) -> List[Dict[str, Any]]:
        """
        –û—Ç—Ä–∏–º—É—î –≤—Å—ñ endpoints –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö endpoints –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
        """
        try:
            results = self.vector_manager.get_embeddings_for_user(
                user_id=self.user_id, swagger_spec_id=self.swagger_spec_id
            )

            logger.info(f"üìã –û—Ç—Ä–∏–º–∞–Ω–æ {len(results)} endpoints –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}")
            return results

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è endpoints: {e}")
            return []

    def delete_user_embeddings(self) -> bool:
        """
        –í–∏–¥–∞–ª—è—î –≤—Å—ñ embeddings –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

        Returns:
            True —è–∫—â–æ —É—Å–ø—ñ—à–Ω–æ –≤–∏–¥–∞–ª–µ–Ω–æ
        """
        try:
            success = self.vector_manager.delete_embeddings_for_user(
                user_id=self.user_id, swagger_spec_id=self.swagger_spec_id
            )

            if success:
                logger.info(f"‚úÖ –í–∏–¥–∞–ª–µ–Ω–æ embeddings –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}")
            else:
                logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è embeddings –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}")

            return success

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è embeddings: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """
        –û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ embeddings –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ—é
        """
        try:
            stats = self.vector_manager.get_statistics(user_id=self.user_id)
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}: {stats}")
            return stats

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

    def _enhance_chunks_with_gpt(
        self, chunks: List[Dict[str, Any]], swagger_data: Dict[str, Any]
    ) -> tuple[List[Dict[str, Any]], List]:
        """
        –ó–±–∞–≥–∞—á—É—î chunks –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é GPT-–∞–Ω–∞–ª—ñ–∑—É.

        Args:
            chunks: –ë–∞–∑–æ–≤—ñ chunks –≤—ñ–¥ parser
            swagger_data: –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ Swagger

        Returns:
            Tuple: (–ø–æ–∫—Ä–∞—â–µ–Ω—ñ chunks, —Å–ø–∏—Å–æ–∫ GPT –ø—Ä–æ–º–ø—Ç—ñ–≤)
        """
        try:
            from src.gpt_prompt_generator import GPTPromptGenerator

            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ GPT –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            gpt_generator = GPTPromptGenerator()

            # –ì–µ–Ω–µ—Ä—É—î–º–æ –ø—Ä–æ–º–ø—Ç–∏ –¥–ª—è –≤—Å—ñ—Ö endpoint'—ñ–≤
            logger.debug(f"–ü–µ—Ä–µ–¥–∞—î–º–æ swagger_data —Ç–∏–ø—É {type(swagger_data)} –¥–æ GPT generator")
            gpt_prompts = gpt_generator.generate_prompts_from_swagger(swagger_data)

            if not gpt_prompts:
                logger.warning("GPT –Ω–µ –∑–≥–µ–Ω–µ—Ä—É–≤–∞–≤ –ø—Ä–æ–º–ø—Ç—ñ–≤")
                return chunks, []

            enhanced_chunks = []

            for chunk in chunks:
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π GPT –ø—Ä–æ–º–ø—Ç
                matching_prompt = self._find_matching_gpt_prompt(chunk, gpt_prompts)

                if matching_prompt:
                    # –ó–±–∞–≥–∞—á—É—î–º–æ chunk GPT insights
                    enhanced_text = self._create_enhanced_chunk_text(chunk, matching_prompt)

                    chunk["text"] = enhanced_text
                    chunk["metadata"]["gpt_enhanced"] = True
                    chunk["metadata"]["gpt_prompt_id"] = matching_prompt.id
                    chunk["metadata"]["gpt_insights"] = True

                enhanced_chunks.append(chunk)

            logger.info(
                f"‚ú® GPT –∑–±–∞–≥–∞—Ç–∏–≤ {len([c for c in enhanced_chunks if c['metadata'].get('gpt_enhanced')])} chunks"
            )
            return enhanced_chunks, gpt_prompts

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ GPT –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {e}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")
            return chunks, []

    def _find_matching_gpt_prompt(self, chunk: Dict[str, Any], gpt_prompts: List) -> any:
        """–ó–Ω–∞—Ö–æ–¥–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–π GPT –ø—Ä–æ–º–ø—Ç –¥–ª—è chunk."""
        try:
            chunk_path = chunk["metadata"].get("path", "")
            chunk_method = chunk["metadata"].get("method", "").upper()

            for prompt in gpt_prompts:
                if (
                    prompt.endpoint_path == chunk_path
                    and prompt.http_method.upper() == chunk_method
                ):
                    return prompt

            return None

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É GPT –ø—Ä–æ–º–ø—Ç—É: {e}")
            return None

    def _create_enhanced_chunk_text(self, chunk: Dict[str, Any], gpt_prompt) -> str:
        """–°—Ç–≤–æ—Ä—é—î –∑–±–∞–≥–∞—á–µ–Ω–∏–π —Ç–µ–∫—Å—Ç chunk –∑ GPT insights."""
        try:
            original_text = chunk["text"]

            # –î–æ–¥–∞—î–º–æ GPT –∞–Ω–∞–ª—ñ–∑ —Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            enhanced_parts = [
                original_text,
                "\n--- GPT Analysis ---",
                f"Smart Description: {gpt_prompt.description}",
                (
                    f"Use Cases: {gpt_prompt.template[:200]}..."
                    if len(gpt_prompt.template) > 200
                    else gpt_prompt.template
                ),
                f"Resource Type: {gpt_prompt.resource_type}",
                f"Category: {gpt_prompt.category}",
            ]

            if hasattr(gpt_prompt, "tags") and gpt_prompt.tags:
                enhanced_parts.append(f"Tags: {', '.join(gpt_prompt.tags)}")

            return "\n".join(enhanced_parts)

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–±–∞–≥–∞—á–µ–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É: {e}")
            return chunk["text"]

    def _save_gpt_prompts(self, gpt_prompts: List) -> bool:
        """
        –ó–±–µ—Ä—ñ–≥–∞—î GPT-–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –ø—Ä–æ–º–ø—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–∏—Ö.

        Args:
            gpt_prompts: –°–ø–∏—Å–æ–∫ GPT –ø—Ä–æ–º–ø—Ç—ñ–≤

        Returns:
            True —è–∫—â–æ —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–æ
        """
        try:
            import uuid
            from datetime import datetime

            from api.database import get_db
            from api.models import PromptTemplate

            # –û—Ç—Ä–∏–º—É—î–º–æ —Å–µ—Å—ñ—é –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
            db = next(get_db())

            saved_count = 0

            for gpt_prompt in gpt_prompts:
                try:
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π –ø—Ä–æ–º–ø—Ç
                    db_prompt = PromptTemplate(
                        id=str(uuid.uuid4()),
                        user_id=self.user_id,
                        swagger_spec_id=self.swagger_spec_id,
                        name=gpt_prompt.name,
                        description=gpt_prompt.description,
                        template=gpt_prompt.template,
                        category=gpt_prompt.category,
                        endpoint_path=gpt_prompt.endpoint_path,
                        http_method=gpt_prompt.http_method,
                        resource_type=gpt_prompt.resource_type,
                        tags=gpt_prompt.tags if hasattr(gpt_prompt, "tags") else [],
                        source="gpt_generated",
                        priority=getattr(gpt_prompt, "priority", 1),
                        is_public=False,
                        is_active=True,
                        usage_count=0,
                        success_rate=0,
                        created_at=datetime.now(),
                        updated_at=datetime.now(),
                    )

                    db.add(db_prompt)
                    saved_count += 1

                except Exception as e:
                    logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ –ø—Ä–æ–º–ø—Ç {gpt_prompt.name}: {e}")
                    continue

            db.commit()
            db.close()

            logger.info(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ {saved_count} GPT –ø—Ä–æ–º–ø—Ç—ñ–≤ –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ {self.user_id}")
            return True

        except Exception as e:
            logger.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è GPT –ø—Ä–æ–º–ø—Ç—ñ–≤: {e}")
            return False
